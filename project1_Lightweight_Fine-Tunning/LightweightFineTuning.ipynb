{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "- **PEFT technique**: LoRA to fine-tune MobileBERT with fewer parameters.\n",
    "\n",
    "- **Model**: distilBERT, smaller version of BERT, chosen for memory efficiency.\n",
    "\n",
    "- **Evaluation approach**: Accuracy is the primary evaluation metric, with loss also monitored to track model fitting. Evaluation speed (samples/steps per second) is tracked to assess efficiency.\n",
    "\n",
    "- **Fine-tuning dataset**: IMDb dataset chosen for simplicity to train and evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb2927",
   "metadata": {},
   "source": [
    "### Loading Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19b4023ff1a4c4c8038f8634c0f08d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 33.5MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 44.2MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:00<00:00, 49.9MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ab8413ff9a4f1d97610e7940151c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0308b0e116b045e9b7a4be9d813c29cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9cf233727c4920b61ee8d580244def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef13a999",
   "metadata": {},
   "source": [
    "###  Dataset Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bed0dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subset label distribution: [2500 2500]\n",
      "Test subset label distribution: [500 500]\n"
     ]
    }
   ],
   "source": [
    "# Create a properly balanced subset of the full dataset\n",
    "def create_balanced_subset(dataset_split, n_samples):\n",
    "    pos_indices = [i for i, label in enumerate(dataset_split['label']) if label == 1]\n",
    "    neg_indices = [i for i, label in enumerate(dataset_split['label']) if label == 0]\n",
    "    \n",
    "    np.random.shuffle(pos_indices)\n",
    "    np.random.shuffle(neg_indices)\n",
    "    \n",
    "    n_per_class = n_samples // 2\n",
    "    selected_indices = pos_indices[:n_per_class] + neg_indices[:n_per_class]\n",
    "    \n",
    "    np.random.shuffle(selected_indices)\n",
    "    \n",
    "    return dataset_split.select(selected_indices)\n",
    "\n",
    "# Create balanced subsets\n",
    "train_subset = create_balanced_subset(dataset['train'], 5000)\n",
    "test_subset = create_balanced_subset(dataset['test'], 1000)\n",
    "\n",
    "# Verify balanced distribution\n",
    "print(f\"Train subset label distribution: {np.bincount(train_subset['label'])}\")\n",
    "print(f\"Test subset label distribution: {np.bincount(test_subset['label'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864fb64b",
   "metadata": {},
   "source": [
    "### Tokenizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95cf43aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315ede7353704785b1a0822d26aedc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058de4a68e2342dfad978b6e0ed372e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1f4f22124941aab17d7056bff43d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf1ce82aa74441b9650dd566290e5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load DistilBERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e9fd81bf81406cad058b0625a9c6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7960b44540374a75b6d974ffc60222e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize data\n",
    "tokenized_train = train_subset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_subset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function with detailed logging\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    pos_pred = np.sum(predictions == 1)\n",
    "    neg_pred = np.sum(predictions == 0)\n",
    "    \n",
    "    # Print detailed stats\n",
    "    print(f\"Predictions distribution: Positive={pos_pred}, Negative={neg_pred}\")\n",
    "    print(f\"First 10 predictions: {predictions[:10]}\")\n",
    "    print(f\"First 10 labels: {labels[:10]}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"pos_ratio\": pos_pred / len(predictions),\n",
    "        \"neg_ratio\": neg_pred / len(predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52441b43",
   "metadata": {},
   "source": [
    "### Evaluate Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bfec4f18e5425e808f34156ea7243b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating base model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions distribution: Positive=562, Negative=438\n",
      "First 10 predictions: [1 0 0 1 1 1 1 1 0 0]\n",
      "First 10 labels: [0 0 0 0 0 0 1 0 0 1]\n",
      "Base Model Evaluation Results: {'eval_loss': 0.6965502500534058, 'eval_accuracy': 0.414, 'eval_pos_ratio': 0.562, 'eval_neg_ratio': 0.438, 'eval_runtime': 15.6483, 'eval_samples_per_second': 63.905, 'eval_steps_per_second': 2.045}\n"
     ]
    }
   ],
   "source": [
    "# Initialize model from scratch\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Set up evaluation args\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./tmp/base_model_eval\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Set up evaluator for base model\n",
    "base_evaluator = Trainer(\n",
    "    model=base_model,\n",
    "    args=eval_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "\n",
    "# Evaluate base model\n",
    "print(\"Evaluating base model...\")\n",
    "base_results = base_evaluator.evaluate()\n",
    "print(\"Base Model Evaluation Results:\", base_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fe560",
   "metadata": {},
   "source": [
    "### Base model evaluation results:\n",
    "- **Accuracy (41.4%)**: The base model's accuracy is below 50%, which is worse than random guessing on this balanced dataset. This indicates that the pre-trained model, without task-specific fine-tuning, is performing poorly on sentiment classification.\n",
    "\n",
    "- **Class Distribution Bias**: The model is predicting positive reviews 56.2% of the time and negative reviews 43.8% of the time, despite our test set being perfectly balanced (500 positive, 500 negative). This shows the model has a slight bias toward predicting positive sentiment.\n",
    "\n",
    "- **First 10 Predictions vs Labels**: Looking at the first 10 examples, the model gets many wrong (e.g., predicting 1 when the label is 0), confirming the poor accuracy.\n",
    "\n",
    "- **Loss Value (0.697)**: This is a reasonable cross-entropy loss value for a classification task, suggesting the model is producing probabilities that are uncertain but not wildly miscalibrated.\n",
    "\n",
    "In short what these results tell us is that the pre-trained DistilBERT model, without any fine-tuning on sentiment analysis tasks, performs worse than random guessing on IMDB reviews. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Create PEFT configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],  \n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c438d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Apply PEFT to a fresh model\n",
    "fresh_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2\n",
    ")\n",
    "peft_model = get_peft_model(fresh_model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6222de1",
   "metadata": {},
   "source": [
    "### Setting up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/distilbert_lora\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,  \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=1e-4,  \n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Set up trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [939/939 09:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Pos Ratio</th>\n",
       "      <th>Neg Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.299100</td>\n",
       "      <td>0.256404</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>0.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>0.240817</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>0.242268</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>0.507000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions distribution: Positive=507, Negative=493\n",
      "First 10 predictions: [0 1 0 0 0 0 1 0 0 1]\n",
      "First 10 labels: [0 0 0 0 0 0 1 0 0 1]\n",
      "Predictions distribution: Positive=490, Negative=510\n",
      "First 10 predictions: [0 0 0 0 0 0 1 0 0 1]\n",
      "First 10 labels: [0 0 0 0 0 0 1 0 0 1]\n",
      "Predictions distribution: Positive=493, Negative=507\n",
      "First 10 predictions: [0 0 0 0 0 0 1 0 0 1]\n",
      "First 10 labels: [0 0 0 0 0 0 1 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions distribution: Positive=493, Negative=507\n",
      "First 10 predictions: [0 0 0 0 0 0 1 0 0 1]\n",
      "First 10 labels: [0 0 0 0 0 0 1 0 0 1]\n",
      "Fine-tuned Model Results (right after training): {'eval_loss': 0.24226754903793335, 'eval_accuracy': 0.903, 'eval_pos_ratio': 0.493, 'eval_neg_ratio': 0.507, 'eval_runtime': 16.5517, 'eval_samples_per_second': 60.417, 'eval_steps_per_second': 1.933, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "trainer.train()\n",
    "\n",
    "ft_results_after_training = trainer.evaluate()\n",
    "print(\"Fine-tuned Model Results (right after training):\", ft_results_after_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639955b",
   "metadata": {},
   "source": [
    "### PEFT model evaluation results:\n",
    "Based on these training and evaluation results for DistilBERT model with LoRA fine-tuning the IMDb dataset, we can observe these improvements:\n",
    "\n",
    "- **Significant Performance Improvement**: The model showed excellent progress from the base model, which had only 41.4% accuracy, to the fine-tuned model reaching 90.3% accuracy after 3 epochs. This is a dramatic improvement of nearly 49 percentage points.\n",
    "- **Consistent Learning Progress**: The accuracy improved across epochs:\n",
    "    - Epoch 1: 88.9%\n",
    "    - Epoch 2: 90.0%\n",
    "    - Epoch 3: 90.3%\n",
    "    This consistent improvement suggests the training was effective and stable.\n",
    "- **Balanced Predictions**: The final predictions are well-balanced between positive (493) and negative (507) classes, this tells us that the model isn't biased and predicting one class over the other.\n",
    "- **Low Loss Values**: The validation is significantly lower (0.24) than the base model (0.69).\n",
    "- **First 10 Predictions Analysis**: The first 10 predictions match the labels almost perfectly by the final epoch, which is another positive sign of the model's performance.\n",
    "- **Effective LoRA Adaptation**: The results demonstrate that LoRA is working effectively for this task, fine-tuning a relatively small number of parameters while showing great performance.\n",
    "\n",
    "In short, the LoRA fine-tuning approach was highly successful for this sentiment analysis task. The model efficiently learned the task patterns with minimal parameter tuning achieving over 90% accuracy, this is the power of LoRA!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/tmp/peft_distilbert/tokenizer_config.json',\n",
       " '/tmp/peft_distilbert/special_tokens_map.json',\n",
       " '/tmp/peft_distilbert/vocab.txt',\n",
       " '/tmp/peft_distilbert/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model and tokenizer\n",
    "peft_model.save_pretrained(\"/tmp/peft_distilbert\")\n",
    "\n",
    "tokenizer.save_pretrained(\"/tmp/peft_distilbert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vocab.txt', 'adapter_model.bin', 'adapter_config.json', 'README.md', 'tokenizer_config.json', 'special_tokens_map.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(\"/tmp/peft_distilbert\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a fresh base model for comparison\n",
    "base_model_for_inference = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "561300aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up base model trainer for inference\n",
    "base_trainer_inference = Trainer(\n",
    "    model=base_model_for_inference,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a8d66c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the saved PEFT model \n",
    "base_model_for_peft = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2789db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# load the adapter \n",
    "peft_model_for_inference = PeftModel.from_pretrained(\n",
    "    base_model_for_peft,\n",
    "    \"/tmp/peft_distilbert\",\n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "# Set up PEFT model trainer for inference\n",
    "peft_trainer_inference = Trainer(\n",
    "    model=peft_model_for_inference,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenized_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating base model during inference...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions distribution: Positive=2, Negative=998\n",
      "First 10 predictions: [0 0 0 0 0 0 0 0 0 0]\n",
      "First 10 labels: [0 0 0 0 0 0 1 0 0 1]\n",
      "\n",
      "Evaluating fine-tuned PEFT model during inference...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions distribution: Positive=493, Negative=507\n",
      "First 10 predictions: [0 0 0 0 0 0 1 0 0 1]\n",
      "First 10 labels: [0 0 0 0 0 0 1 0 0 1]\n",
      "\n",
      "===== Final Evaluation Results =====\n",
      "Base Model Accuracy: 0.5000\n",
      "Fine-tuned PEFT Model Accuracy: 0.9030\n",
      "Improvement: 0.4030\n"
     ]
    }
   ],
   "source": [
    "# Evaluate both models\n",
    "print(\"\\nEvaluating base model during inference...\")\n",
    "base_inference_results = base_trainer_inference.evaluate()\n",
    "\n",
    "print(\"\\nEvaluating fine-tuned PEFT model during inference...\")\n",
    "peft_inference_results = peft_trainer_inference.evaluate()\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n===== Final Evaluation Results =====\")\n",
    "print(f\"Base Model Accuracy: {base_inference_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Fine-tuned PEFT Model Accuracy: {peft_inference_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Improvement: {peft_inference_results['eval_accuracy'] - base_inference_results['eval_accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68f7b9f9",
   "metadata": {},
   "source": [
    "### Trained PEFT model performance:\n",
    "These results validate the effectiveness of this LoRA fine-tuning approach. Here’s a breakdown of the improvements:\n",
    "\n",
    "- **Base Model Performance**: During inference the base model shows a highly skewed prediction distribution (998 negative, only 2 positive predictions). It’s basically defaulting to negative predictions and the accuracy of 50% means the base model is random guessing the classification task. \n",
    "- **Fine-tuned Model Success**: The LoRA fine-tuned model shows excellent performance with 90.3% accuracy, exactly matching what we got during training, confirming the PEFT approach worked as intended.\n",
    "- **Balanced Predictions from Fine-tuned Model**: The fine-tuned model produces a balanced distribution of predictions (493 positive, 507 negative) meaning the model is making nuanced decisions rather than defaulting to one class.\n",
    "- **Substantial Improvement**: We achieved 40.3 percentage point improvement! \n",
    "- **First 10 Predictions**: The fine-tuned model's first 10 predictions perfectly match the labels, while the base model incorrectly predicted all of them as negative.\n",
    "- **Effective Parameter Efficiency**: The entire DistilBERT model wasn't fine-tuned during LoRA just a few injected adapters got the job done.\n",
    "\n",
    "These results represent a substantial improvement over the base model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438c43b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
